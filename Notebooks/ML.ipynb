{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "import natsort\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mearly_stopping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiceLoss\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import rioxarray as xr\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from early_stopping import EarlyStopping\n",
    "from utils import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'RESIZE_WIDTH':32,\n",
    "    'RESIZE_HEIGHT':32,\n",
    "    'EPOCHS':50,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':41,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = './train/s2_image/'\n",
    "mask_dir = './train/mask/'\n",
    "img_pattern = '*.tif'\n",
    "mask_pattern = '*.tif'\n",
    "\n",
    "# Glob and sort image and mask paths\n",
    "train_img_paths = natsort.natsorted(glob.glob(img_dir + img_pattern))\n",
    "train_mask_paths = natsort.natsorted(glob.glob(mask_dir + mask_pattern))\n",
    "\n",
    "# Create a DataFrame\n",
    "total_df = pd.DataFrame({'img_path': train_img_paths, 'mask_path': train_mask_paths})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid= train_test_split(total_df, test_size=0.2, random_state=CFG['SEED'])\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_valid = X_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, train_mode=True, transforms=None):\n",
    "        self.image = df['img_path']\n",
    "        self.mask = df['mask_path']\n",
    "\n",
    "        self.train_mode = train_mode\n",
    "        self.transforms = transforms\n",
    "        self.img_list = self.feature_func(self.image)\n",
    "        self.mask_list = self.mask_func(self.mask)\n",
    "   \n",
    "    def feature_func(self, df_path):\n",
    "        img_list=[]\n",
    "        for img_dir in tqdm(df_path):\n",
    "            with rasterio.open(img_dir) as image:\n",
    "                image_array = image.read() #(12,24,33)\n",
    "                image_array = np.transpose(image_array, (1,2,0)) #(32,32,12)\n",
    "                image_array = cv2.resize(image_array, (CFG['RESIZE_WIDTH'], CFG['RESIZE_HEIGHT']))   #(12,32,32)\n",
    "                image_array = np.transpose(image_array, (2,0,1)) #(24,33,12)\n",
    "                tr_img = torch.FloatTensor(image_array)\n",
    "                img_list.append(tr_img)\n",
    "        return np.array(img_list)\n",
    "    \n",
    "    def mask_func(self, df_path):\n",
    "        mask_list=[]\n",
    "        for img_dir in tqdm(df_path):\n",
    "            with rasterio.open(img_dir) as image:\n",
    "                image_array = image.read() #(1,24,24)\n",
    "                image_array = np.transpose(image_array, (1,2,0)) #(32,32,12)\n",
    "                image_array = cv2.resize(image_array, (CFG['RESIZE_WIDTH'], CFG['RESIZE_HEIGHT']))   #(12,32,32)\n",
    "                image_array = np.reshape(image_array, (1, image_array.shape[0],image_array.shape[1])) #(1,32,32)\n",
    "                mask = torch.FloatTensor(image_array)\n",
    "\n",
    "                mask_list.append(mask)\n",
    "        return np.array(mask_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_feature = self.img_list[index]\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            X_feature = self.transforms(X_feature)\n",
    "        \n",
    "        if self.train_mode:\n",
    "            label = self.mask_list[index]\n",
    "            return X_feature, label\n",
    "        else:\n",
    "            return X_feature\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:05<00:00, 151.66it/s]\n",
      "100%|██████████| 800/800 [00:05<00:00, 158.09it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 150.13it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 165.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, train_mode=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "vali_dataset = CustomDataset(X_valid, train_mode=True)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(vali_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 655.5820,  657.3223,  658.5923,  ...,  779.1923,  785.4283,\n",
       "           793.2841],\n",
       "         [ 655.6202,  657.4747,  658.8280,  ...,  779.2706,  785.5397,\n",
       "           793.3956],\n",
       "         [ 656.3389,  657.8773,  659.0000,  ...,  779.4662,  785.6343,\n",
       "           793.4771],\n",
       "         ...,\n",
       "         [ 367.5000,  367.5000,  367.5000,  ...,  682.0000,  677.9036,\n",
       "           672.2901],\n",
       "         [ 367.5000,  367.5000,  367.5000,  ...,  682.0000,  677.7332,\n",
       "           671.8860],\n",
       "         [ 367.5000,  367.5000,  367.5000,  ...,  682.0000,  677.4995,\n",
       "           671.3322]],\n",
       "\n",
       "        [[1102.5396, 1045.2802,  959.6609,  ..., 1194.6150, 1116.6873,\n",
       "          1021.5367],\n",
       "         [1071.8906, 1036.2933,  966.7753,  ..., 1067.0142, 1024.7092,\n",
       "           965.8724],\n",
       "         [1033.1443, 1027.3279,  977.2184,  ...,  908.0965,  898.8860,\n",
       "           875.2604],\n",
       "         ...,\n",
       "         [ 215.0388,  237.1836,  268.0695,  ...,  936.6506,  749.6335,\n",
       "           555.6659],\n",
       "         [ 217.4013,  237.2947,  266.9336,  ...,  811.7624,  645.7416,\n",
       "           500.1445],\n",
       "         [ 222.8319,  242.1041,  272.1333,  ...,  672.5411,  542.4157,\n",
       "           458.3507]],\n",
       "\n",
       "        [[1498.5338, 1433.5907, 1323.4597,  ..., 1276.7880, 1211.2981,\n",
       "          1135.6444],\n",
       "         [1463.4254, 1422.8940, 1324.0864,  ..., 1210.6648, 1167.5088,\n",
       "          1116.1520],\n",
       "         [1413.6128, 1406.3934, 1328.8268,  ..., 1114.4733, 1095.1975,\n",
       "          1070.7991],\n",
       "         ...,\n",
       "         [ 374.0152,  429.0414,  486.0439,  ..., 1131.1385,  996.0564,\n",
       "           850.8968],\n",
       "         [ 394.6751,  450.2213,  508.0870,  ..., 1025.1422,  917.6323,\n",
       "           824.6296],\n",
       "         [ 419.9716,  481.2532,  542.7063,  ...,  921.9327,  853.7560,\n",
       "           817.4444]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2464.6384, 2487.4805, 2504.1487,  ..., 2313.3557, 2296.8552,\n",
       "          2279.2454],\n",
       "         [2465.1396, 2489.4805, 2507.2424,  ..., 2312.7883, 2296.0720,\n",
       "          2278.4941],\n",
       "         [2474.5725, 2494.7649, 2509.5000,  ..., 2311.3699, 2295.4038,\n",
       "          2277.9458],\n",
       "         ...,\n",
       "         [2750.5000, 2750.5000, 2750.5000,  ..., 2506.0000, 2524.3552,\n",
       "          2549.5088],\n",
       "         [2750.5000, 2750.5000, 2750.5000,  ..., 2506.0000, 2523.5110,\n",
       "          2547.5073],\n",
       "         [2750.5000, 2750.5000, 2750.5000,  ..., 2506.0000, 2522.3538,\n",
       "          2544.7644]],\n",
       "\n",
       "        [[2803.4717, 2762.3831, 2732.3997,  ..., 1948.9889, 1952.3773,\n",
       "          1953.4071],\n",
       "         [2800.7654, 2770.9675, 2749.2231,  ..., 1953.6056, 1956.8102,\n",
       "          1957.6787],\n",
       "         [2802.0173, 2784.2529, 2771.2896,  ..., 1953.8159, 1961.0623,\n",
       "          1963.1029],\n",
       "         ...,\n",
       "         [1415.8677, 1663.6708, 1844.5000,  ..., 2340.0505, 2402.7651,\n",
       "          2405.3594],\n",
       "         [1455.6317, 1723.6423, 1919.2178,  ..., 2337.8745, 2374.3318,\n",
       "          2373.6826],\n",
       "         [1510.1232, 1805.8258, 2021.6088,  ..., 2334.8926, 2335.3677,\n",
       "          2330.2734]],\n",
       "\n",
       "        [[2060.7295, 1986.2488, 1931.8982,  ..., 1402.2195, 1422.6097,\n",
       "          1431.0427],\n",
       "         [2064.1013, 1993.8885, 1942.6523,  ..., 1407.4088, 1427.1838,\n",
       "          1435.2830],\n",
       "         [2040.8318, 1995.4003, 1962.2474,  ..., 1367.7733, 1387.8495,\n",
       "          1396.0996],\n",
       "         ...,\n",
       "         [ 802.4849, 1064.0952, 1255.0000,  ..., 1656.4508, 1666.0913,\n",
       "          1650.1113],\n",
       "         [ 798.5549, 1072.8169, 1272.9540,  ..., 1607.9376, 1608.7122,\n",
       "          1592.7028],\n",
       "         [ 793.1693, 1084.7688, 1297.5577,  ..., 1541.4565, 1530.0814,\n",
       "          1514.0319]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 32, 32]), torch.Size([1, 32, 32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vali_dataset[0][0].shape, vali_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = smp.FPN(\n",
    "    encoder_name=\"resnet18\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    # use `imagenet` pre-trained weights for encoder initialization\n",
    "    # encoder_weights=\"imagenet\",\n",
    "    # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    in_channels=12,\n",
    "    # model output channels (number of classes in your dataset)\n",
    "    classes=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss().to(device)\n",
    "criterion = DiceLoss().to(device)\n",
    "optimizer = torch.optim.SGD(params=seg_model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3,threshold_mode='abs',min_lr=1e-8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_f1_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(iter(val_loader)):\n",
    "            img, label = img.float().to(device), label.to(device)\n",
    "\n",
    "            logit = model(img)\n",
    "            logit = logit/255\n",
    "            loss = criterion(logit, label)\n",
    "            \n",
    "            logit = logit.detach().cpu().numpy()\n",
    "            label = label.detach().cpu().numpy()\n",
    "            seg_target = label[0,:,:].flatten()\n",
    "            threshold = 0.5\n",
    "            binary_prediction = (logit[0,0,:,:] > threshold).astype(np.uint8)\n",
    "            binary_prediction = binary_prediction.flatten()\n",
    "            val_f1 = f1_score(seg_target, binary_prediction)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            val_f1_list.append(val_f1)\n",
    "\n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_f1 = np.mean(val_f1_list)\n",
    "        \n",
    "    return _val_loss, _val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, optimizer, train_loader, scheduler, device): \n",
    "    model.to(device)\n",
    "    total_train_loss, total_valid_loss = [],[]\n",
    "    total_train_f1, total_valid_f1 = [],[]\n",
    "\n",
    "    patience = 5\n",
    "    early_stopping = EarlyStopping(patience = patience, path = f'./weights/best.pt', verbose = True)\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS']):  # Adjusted to include the last epoch\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_f1_list = []\n",
    "        for img, label in tqdm(iter(train_loader)):\n",
    "            img, label = img.float().to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(img)\n",
    "            logit = logit/255\n",
    "            loss = criterion(logit, label)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            logit = logit.detach().cpu().numpy()\n",
    "            label = label.detach().cpu().numpy()\n",
    "            seg_target = label[0,:,:].flatten()\n",
    "            threshold = 0.5\n",
    "            binary_prediction = (logit[0,0,:,:] > threshold).astype(np.uint8)\n",
    "            binary_prediction = binary_prediction.flatten()\n",
    "            train_f1 = f1_score(seg_target, binary_prediction)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_f1_list.append(train_f1)\n",
    "\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        _train_f1 = np.mean(train_f1_list)\n",
    "\n",
    "        _val_loss, _val_f1 = validation(model, criterion, vali_loader, device)\n",
    "        total_train_loss.append(_train_loss)\n",
    "        total_valid_loss.append(_val_loss)\n",
    "        total_train_f1.append(_train_f1)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Train F1 : [{_train_f1:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_f1:.5f}]]')\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "\n",
    "        early_stopping(_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return total_train_loss, total_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dc716e73b74f8e8dca087a08e58d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bf7b4c566543b6a912a3e012389084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.94179] Train F1 : [0.00000] Val Loss : [0.94573] Val F1 : [0.00000]]\n",
      "Validation loss decreased (inf --> 0.945726).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0051f0118e4b69b753f8d387849c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17e59056e344dc19c0fdfd5131627d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.94230] Train F1 : [0.00000] Val Loss : [0.94497] Val F1 : [0.00000]]\n",
      "Validation loss decreased (0.945726 --> 0.944967).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61753399018a4fd18d0b37732781c2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253eb99af9cb4264bf0fe6a551617255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.94070] Train F1 : [0.00000] Val Loss : [0.94314] Val F1 : [0.00000]]\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Validation loss decreased (0.944967 --> 0.943135).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c1956a6dd3429680a829d7ef905880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\SOLAFUNE\\BASELINE.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/SOLAFUNE/BASELINE.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(seg_model, optimizer, train_loader, scheduler, device)\n",
      "\u001b[1;32mc:\\Users\\ok\\Desktop\\SOLAFUNE\\BASELINE.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/SOLAFUNE/BASELINE.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m logit \u001b[39m=\u001b[39m logit\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/SOLAFUNE/BASELINE.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(logit, label)  \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/SOLAFUNE/BASELINE.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/SOLAFUNE/BASELINE.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ok/Desktop/SOLAFUNE/BASELINE.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m logit \u001b[39m=\u001b[39m logit\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\ok\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ok\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(seg_model, optimizer, train_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
